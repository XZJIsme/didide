# 保护地得，从我做起
## Get started now
```
conda create -n huggingface python=3.10 -y
conda activate huggingface
conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.3 -c pytorch -y
pip install transformers datasets
python model_training.py
```
## Convert TensorFlow pre-trained BERT models to PyTorch
```
conda install tensorflow -y
mkdir BERT-trained
cd BERT-trained
wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip
unzip chinese_L-12_H-768_A-12.zip
cd ..
python bert_converter.py
```
## Generate DidideData
```python
# python dataset_generator.py
"""
Here we use wiki_zh from https://github.com/brightmart/nlp_chinese_corpus
A sample of processed wiki_zh data is in data/samples, named wiki_zh_mini.pkl which contains a list generated by the script.
"""
# for file in glob.glob("yourcorpus/**", recursive=True):
# ↑if you use customized corpus, some modifications are needed, see the easy-to-read script for details.
for file in glob.glob("data/wiki_zh/**", recursive=True):
    if os.path.isdir(file):
        continue
    files.append(file)
```
## Train the model
```
python model_training.py
```
## Give it a try
```
python playground.py --text "我觉的我烦的有点难过，因为我得培根忘记吃了"
```